{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0d4de1b-cc98-45eb-8239-f84ba6812ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a Reddit instance with the following details:\n",
      "Client ID: Vu3sWAZl4FCASbG-b48yVA\n",
      "User Agent:: Kaggle Scraping\n",
      "Started Collecting Data!\n",
      "Post data read\n",
      "The size of the collected dataframe is: (1, 11)\n"
     ]
    }
   ],
   "source": [
    "# %% [code]\n",
    "# Reddit Module to abstract the task of data abstraction \n",
    "\n",
    "# Import PRAW - python reddit API wrapper\n",
    "#!pip install praw\n",
    "#!pip install pandas\n",
    "import praw\n",
    "import pandas as pd\n",
    "\n",
    "# Data features that we will be collecting \n",
    "features = [\n",
    "    'ID', \n",
    "    'is_Original', \n",
    "    'Flair',\n",
    "    'num_comments', \n",
    "    'Title',\n",
    "    'Subreddit', \n",
    "    'Body', \n",
    "    'URL', \n",
    "    'Upvotes',\n",
    "    'created_on', \n",
    "    'Comments'\n",
    "]\n",
    "\n",
    "# Function to authenticate the user \n",
    "def reddit_auth(my_client_id, my_client_secret, user):\n",
    "    '''\n",
    "    INPUTS: \n",
    "    \n",
    "    my_client_id: generated from the Reddit app website\n",
    "    my_client_secret: generated from the Reddit app website\n",
    "    user_agent: generated from the Reddit app website\n",
    "\n",
    "    OUTPUT: \n",
    "\n",
    "    dataframe with the complete data \n",
    "    '''\n",
    "    \n",
    "    reddit_auth_var = praw.Reddit(client_id=my_client_id, \n",
    "                        client_secret=my_client_secret, \n",
    "                        user_agent=user)\n",
    "\n",
    "    print(\"Created a Reddit instance with the following details:\")\n",
    "    print(\"Client ID: {}\\nUser Agent:: {}\".format(my_client_id, user))\n",
    "\n",
    "    # Return the instance \n",
    "    return reddit_auth_var\n",
    "\n",
    "\n",
    "# Function to get the unique flairs \n",
    "def get_unique_flairs(reddit, sub_name, num_posts):\n",
    "    \n",
    "    '''\n",
    "    INPUTS: \n",
    "\n",
    "    reddit: An instance of the Reddit API\n",
    "    subreddit: the subreddit the post belongs to \n",
    "    num_posts: Number of posts to look for \n",
    "\n",
    "    OUTPUT: \n",
    "\n",
    "    dataframe with the complete data \n",
    "\n",
    "    '''\n",
    "    # Getting post data\n",
    "    posts = reddit.subreddit(sub_name).top(limit=num_posts)\n",
    "    # url = \"https://www.reddit.com/r/Games/comments/q9v3mh/weekly_rgames_discussion_what_have_you_been/\"\n",
    "    # posts = reddit.submission(url=url)\n",
    "    \n",
    "    # Collect a unique list of flairs \n",
    "    flairs = []\n",
    "    for post in posts:\n",
    "\n",
    "        # Extract flair name\n",
    "        post_flair = post.link_flair_text\n",
    "        if post_flair != None: \n",
    "\n",
    "            # Check if flair in the list \n",
    "            # Add if not\n",
    "            if post_flair not in flairs:\n",
    "                flairs.append(post_flair)\n",
    "    \n",
    "    # Check if not empty \n",
    "    if len(flairs) != 0: \n",
    "        return flairs\n",
    "    \n",
    "    else:\n",
    "        print(\"No Flairs found.\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "# Function to get the data using particular flairs\n",
    "def scrape_with_flairs(reddit, sub_name, flairs, num_per_flair, features=features, comments=True):\n",
    "    \n",
    "    '''\n",
    "    INPUTS: \n",
    "\n",
    "    reddit: An instance of the Reddit API\n",
    "    subreddit: the subreddit the post belongs to \n",
    "    flairs: list of flairs that you need the data for\n",
    "    num_per_flair: Number of posts per flair \n",
    "    comments: True if you want additional comments and False if you \n",
    "\n",
    "    OUTPUT: \n",
    "\n",
    "    dataframe with the complete data \n",
    "\n",
    "    '''\n",
    "    \n",
    "    # Get comments detail \n",
    "    if comments == True:\n",
    "        comment_limit = None\n",
    "    else: \n",
    "        comment_limit = 0\n",
    "    \n",
    "    \n",
    "    # Create a list which will have each row as an entry  \n",
    "    posts = []\n",
    "    \n",
    "    # Create a subreddit instance \n",
    "    subreddit_total = reddit.subreddit(sub_name)\n",
    "\n",
    "    # Top posts of each flair type based on the number of posts per flair \n",
    "    for flair in flairs: \n",
    "        \n",
    "        print(\"Collecting for flair: {}\".format(flair))\n",
    "        relevant_subs = subreddit_total.search(f\"flair_name:{flair}\", limit=num_per_flair)\n",
    "\n",
    "        for sub in relevant_subs:\n",
    "            post = []\n",
    "            post = [\n",
    "                str(sub.id),\n",
    "                sub.is_original_content,\n",
    "                str(sub.link_flair_text),\n",
    "                sub.num_comments,\n",
    "                str(sub.title),\n",
    "                str(sub.subreddit),\n",
    "                str(sub.selftext),\n",
    "                str(sub.url),\n",
    "                sub.score,\n",
    "                sub.created_utc,\n",
    "            ]\n",
    "            \n",
    "            # Collect comments\n",
    "            sub.comments.replace_more(limit=None)\n",
    "            comment = ''\n",
    "            for top_comment in sub.comments:\n",
    "                comment = str(top_comment.body) + ' '        \n",
    "\n",
    "            post.append(str(comment))# Add to the end of the list \n",
    "            posts.append(post)    # Add to the main list\n",
    "\n",
    "            # Update after every 100 posts\n",
    "            if len(posts) % 100 == 0:\n",
    "                print(\"Number of posts collected: {}\".format(len(posts)))\n",
    "   \n",
    "    # Convert to a data frame \n",
    "    posts_df = pd.DataFrame(posts, columns=features)\n",
    "    print(\"The size of the collected dataframe is: {}\".format(posts_df.shape))\n",
    "    \n",
    "    # Using the to_datetime function of pandas to convert time from UNIX to regular \n",
    "    posts_df['creation_date'] = pd.to_datetime(posts_df['created_on'], dayfirst=True, unit='s')\n",
    "    # Drop created_on column now \n",
    "    posts_df.drop(['created_on'], axis=1, inplace=True)\n",
    "    \n",
    "    # Return the dataframe\n",
    "    return posts_df\n",
    "\n",
    "# Function to collect data without flairs\n",
    "def scrape_without_flairs(reddit, sub_name, num_posts, features=features, comments=True):\n",
    "    \n",
    "    '''\n",
    "    INPUTS: \n",
    "\n",
    "    reddit: An instance of the Reddit API\n",
    "    subreddit: the subreddit the post belongs to \n",
    "    num_posts: Number of posts\n",
    "    comments: True if you want additional comments and False if you \n",
    "\n",
    "    OUTPUT: \n",
    "\n",
    "    dataframe with the complete data \n",
    "\n",
    "    '''\n",
    "    \n",
    "    # Get comments detail \n",
    "    if comments == True:\n",
    "        # comment_limit = None\n",
    "        comment_limit = 50\n",
    "    else: \n",
    "        comment_limit = 0\n",
    "    \n",
    "    print(\"Started Collecting Data!\")\n",
    "    \n",
    "    # Create a list which will have each row as an entry  \n",
    "    posts = []\n",
    "    \n",
    "    # Tag the post we want data from\n",
    "    url = \"https://www.reddit.com/r/Games/comments/q9v3mh/weekly_rgames_discussion_what_have_you_been/\"\n",
    "    sub = reddit.submission(\"q9v3mh\")\n",
    "    \n",
    "    # Create a subreddit instance \n",
    "    # subreddit_total = reddit.subreddit(sub_name)\n",
    "\n",
    "    # Loop through each subreddit entry and append that to the posts list\n",
    "    '''\n",
    "    for sub in subreddit_total.top(limit=num_posts):\n",
    "    '''    \n",
    "    # Empty list to append post data \n",
    "    post = []\n",
    "\n",
    "    post = [\n",
    "        str(sub.id),\n",
    "        sub.is_original_content,\n",
    "        str(sub.link_flair_text),\n",
    "        sub.num_comments,\n",
    "        str(sub.title),\n",
    "        str(sub.subreddit),\n",
    "        str(sub.selftext),\n",
    "        str(sub.url),\n",
    "        sub.score,\n",
    "        sub.created_utc,\n",
    "    ]\n",
    "        \n",
    "        \n",
    "    # print if post data was successfully read\n",
    "    print(\"Post data read\")\n",
    "        \n",
    "    # Collect comments\n",
    "    sub.comments.replace_more(limit=comment_limit)\n",
    "    comment = ''\n",
    "    comment_list = []\n",
    "    \n",
    "        \n",
    "    #for top_comment in sub.comments:\n",
    "    #    comment = str(top_comment.body) + ' '  \n",
    "            \n",
    "    for top_comment in sub.comments:\n",
    "        comment = str(top_comment.body) + ' '\n",
    "        comment_list.append(comment)\n",
    "            \n",
    "    # comment data printing\n",
    "    # print(post)\n",
    "    \n",
    "    post.append(str(comment))# Add to the end of the list \n",
    "    posts.append(post)    # Add to the main list\n",
    "        \n",
    "    # Update after every 100 posts\n",
    "    if len(posts) % 100 == 0:\n",
    "            print(\"Number of posts collected: {}\".format(len(posts)))\n",
    "   \n",
    "    # Convert to a data frame \n",
    "    posts_df = pd.DataFrame(posts, columns=features)\n",
    "    print(\"The size of the collected dataframe is: {}\".format(posts_df.shape))\n",
    "    \n",
    "    # Using the to_datetime function of pandas to convert time from UNIX to regular \n",
    "    posts_df['creation_date'] = pd.to_datetime(posts_df['created_on'], dayfirst=True, unit='s')\n",
    "    # Drop created_on column now \n",
    "    posts_df.drop(['created_on'], axis=1, inplace=True)\n",
    "    \n",
    "    # Return the dataframe and list of comments\n",
    "    return posts_df, comment_list\n",
    "\n",
    "\n",
    "# Main code to run the Web Scraper\n",
    "reddit_instance = reddit_auth(\"confidential\",\"confidential\",\"confidential\")\n",
    "reddit_dataframe, comment_list = scrape_without_flairs(reddit_instance, \"games\", 2)\n",
    "reddit_dataframe.to_csv('reddit_data.csv')\n",
    "\n",
    "comment_dataframe = pd.DataFrame(data={\"Comment Body\": comment_list})\n",
    "comment_dataframe.to_csv(\"Comment Data.csv\", sep=',',index=False)\n",
    "\n",
    "# print test\n",
    "#print(\"Run complete\")\n",
    "#print(\"Comment list: \")\n",
    "#count = 1\n",
    "#for comment in comment_list:\n",
    "#    print(\"----------Comment \" + str(count) + \"----------\\n\" + comment)\n",
    "#    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee36a70-6c8d-448a-b622-0f4abb11f335",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
