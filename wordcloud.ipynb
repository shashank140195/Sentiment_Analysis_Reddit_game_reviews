{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom os import path\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nimport re\nstop_words = set(stopwords.words('english'))\n\ntrain = pd.read_csv(\"../input/dataset/reddit_training.csv\")\ntest = pd.read_csv('../input/dataset/reddit_testing.csv')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-07T02:31:08.602995Z","iopub.execute_input":"2021-12-07T02:31:08.603895Z","iopub.status.idle":"2021-12-07T02:31:10.244622Z","shell.execute_reply.started":"2021-12-07T02:31:08.603765Z","shell.execute_reply":"2021-12-07T02:31:10.243681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['Comment Body'] = train['Comment Body'].str.strip().str.lower()\ntrain['Comment Body'] = train['Comment Body'].str.replace(\"\\n\", \" \")\n#train = train.drop('Overall Sentiment', axis=1)\ntrain = train.drop('Controls', axis=1)\ntrain = train.drop('Graphics', axis=1)\ntrain = train.drop('Story', axis=1)\ntrain = train.drop('Bugs', axis=1)\n\ntest['Comment Body'] = test['Comment Body'].str.strip().str.lower()\ntest['Comment Body'] = test['Comment Body'].str.replace(\"\\n\", \" \")\n#test = test.drop('Overall Sentiment', axis=1)\ntest = test.drop('Controls', axis=1)\ntest = test.drop('Graphics', axis=1)\ntest = test.drop('Story', axis=1)\ntest = test.drop('Bugs', axis=1)\n\ndoc = pd.concat([train, test])\ndoc['Overall Sentiment'] = doc['Overall Sentiment'].str.strip().str.lower()\ndoc = doc[doc['Overall Sentiment'] != 'neutral']\ndoc['Comment Body'] = doc['Comment Body'].str.replace(\"game\", \"\")\ndoc['Comment Body'] = doc['Comment Body'].str.replace(\"time\", \"\")\ndoc['Comment Body'] = doc['Comment Body'].str.replace(\"one\", \"\")\ndoc['Comment Body'] = doc['Comment Body'].str.replace(\"play\", \"\")\ndoc['Comment Body'] = doc['Comment Body'].str.replace(\"still\", \"\")\ndoc['Comment Body'] = doc['Comment Body'].str.replace(\"many\", \"\")\ndocPositive = doc[doc['Overall Sentiment'] == 'positive']\ndocNegative = doc[doc['Overall Sentiment'] == 'negative']\ndocPositive = docPositive.drop('Overall Sentiment', axis=1)\ndocNegative = docNegative.drop('Overall Sentiment', axis=1)\ndocNegative['Comment Body'] = docNegative['Comment Body'].str.replace(\"good\", \"\")\ndocNegative['Comment Body'] = docNegative['Comment Body'].str.replace(\"great\", \"\")\ndocNegative['Comment Body'] = docNegative['Comment Body'].str.replace(\"next\", \"\")\ndocNegative['Comment Body'] = docNegative['Comment Body'].str.replace(\"much\", \"\")\ndocNegative['Comment Body'] = docNegative['Comment Body'].str.replace(\"new\", \"\")\ndocNegative['Comment Body'] = docNegative['Comment Body'].str.replace(\"able\", \"\")\ndocPositive['Comment Body'] = docPositive['Comment Body'].str.replace(\"bad\", \"\")\n\ntext = ''\n\nfor i in  docPositive['Comment Body']:\n    text = text + i\n    \ntext\n\ntokenized_document = re.split(r'[\\n\\t\\s.,;:''\"\"()?!\\\\-*]', text)\ntokenized_document = [i for i in tokenized_document if i]\n\nadjectives = []\nfor j in tokenized_document:\n      \n    # Word tokenizers is used to find the words \n    # and punctuation in a string\n    wordsList = nltk.word_tokenize(j)\n    \n    # removing stop words from wordList\n    wordsList = [w for w in wordsList if not w in stop_words]\n  \n    #  Using a Tagger. Which is part-of-speech \n    # tagger or POS-tagger. \n    if len(wordsList)!= 0:\n        tagged = nltk.pos_tag(wordsList)\n    \n    if tagged[0][1].startswith(\"JJ\"):\n        adjectives.append(tagged)\n     \n    \n#print(adjectives)\ndocument = ''\n\nfor adj in adjectives:\n    abc = adj[0][0]\n    document = document + ' ' + abc\n     \nword_cloud = WordCloud(collocations = False, background_color = 'white').generate(document)\nplt.imshow(word_cloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-12-07T02:31:10.246339Z","iopub.execute_input":"2021-12-07T02:31:10.246582Z","iopub.status.idle":"2021-12-07T02:31:17.969144Z","shell.execute_reply.started":"2021-12-07T02:31:10.246554Z","shell.execute_reply":"2021-12-07T02:31:17.967841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text1 = ''\n\nfor i in  docNegative['Comment Body']:\n    text1 = text1 + i\n\ntokenized_document1 = re.split(r'[\\n\\t\\s.,;:''\"\"()?!\\\\-*]', text1)\ntokenized_document1 = [i for i in tokenized_document1 if i]\n\nadjectives1 = []\nfor j in tokenized_document1:\n      \n    # Word tokenizers is used to find the words \n    # and punctuation in a string\n    wordsList = nltk.word_tokenize(j)\n    \n    # removing stop words from wordList\n    wordsList = [w for w in wordsList if not w in stop_words]\n  \n    #  Using a Tagger. Which is part-of-speech \n    # tagger or POS-tagger. \n    if len(wordsList)!= 0:\n        tagged = nltk.pos_tag(wordsList)\n    \n    if tagged[0][1].startswith(\"JJ\"):\n        adjectives1.append(tagged)\n     \n    \n#print(adjectives)\ndocument1 = ''\n\nfor adj in adjectives1:\n    abc = adj[0][0]\n    document1 = document1 + ' ' + abc\n     \nword_cloud1 = WordCloud(collocations = False, background_color = 'white').generate(document1)\nplt.imshow(word_cloud1, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-07T02:31:17.970488Z","iopub.execute_input":"2021-12-07T02:31:17.970816Z","iopub.status.idle":"2021-12-07T02:31:20.097659Z","shell.execute_reply.started":"2021-12-07T02:31:17.970777Z","shell.execute_reply":"2021-12-07T02:31:20.096367Z"},"trusted":true},"execution_count":null,"outputs":[]}]}